import streamlit as st
import tempfile
import json
import os
from datetime import datetime
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_cohere import ChatCohere, CohereEmbeddings
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_groq import ChatGroq
from langchain_google_genai import GoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
from evaluate import load as load_metric
import pandas as pd
from langchain_community.vectorstores import FAISS

load_dotenv()


COHERE_API_KEY = os.getenv("COHERE_API_KEY")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


llms = {
    "Cohere": ChatCohere(cohere_api_key=COHERE_API_KEY, model="command-a-03-2025", temperature=0),
    "LLaMA": ChatGroq(model="llama-3.1-8b-instant", groq_api_key=GROQ_API_KEY, temperature=0),
    "GPT-4": ChatOpenAI(api_key=OPENAI_API_KEY, model="gpt-4")
}

embeddings = CohereEmbeddings(cohere_api_key=COHERE_API_KEY, model="embed-v4.0")


def get_conversational_chain(llm):
    prompt_template = """You are a highly skilled academic summarizer with extensive experience in distilling complex research papers into clear, concise summaries that highlight key findings and implications.

The summary should include a short introduction, the key findings, and their importance in a non-technical way.

Document Context:
{context}"""
    prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    chain = prompt | llm | StrOutputParser()
    return chain

def run_summarization_with_chain(llm, docs, query):
    chain = get_conversational_chain(llm)
    return chain.invoke({"context": docs, "question": query})

def evaluate_summary(prediction, reference):
    rouge = load_metric("rouge")
    bleu = load_metric("bleu")
    meteor = load_metric("meteor")

    rouge_result = rouge.compute(predictions=[prediction], references=[reference])
    bleu_result = bleu.compute(predictions=[prediction], references=[reference])
    meteor_result = meteor.compute(predictions=[prediction], references=[reference])

    return {
        "ROUGE": rouge_result,
        "BLEU": bleu_result,
        "METEOR": meteor_result
    }

def format_metrics_for_display(evaluation_results):
    return pd.DataFrame([
        {
            "Model": name,
            "ROUGE-1": metrics["ROUGE"]["rouge1"],
            "ROUGE-L": metrics["ROUGE"]["rougeL"],
            "BLEU": metrics["BLEU"]["bleu"],
            "METEOR": metrics["METEOR"]["meteor"],
        }
        for name, metrics in evaluation_results.items()
    ])

def interpret_metrics_with_llm(metrics_dict, llm):
    formatted_scores = ""
    for model, metrics in metrics_dict.items():
        rouge = metrics["ROUGE"]["rouge1"]
        bleu = metrics["BLEU"]["bleu"]
        meteor = metrics["METEOR"]["meteor"]
        formatted_scores += (
            f"{model}:\n"
            f"  ROUGE-1: {rouge:.3f}\n"
            f"  BLEU: {bleu:.3f}\n"
            f"  METEOR: {meteor:.3f}\n\n"
        )

    prompt = f"""
Here are the evaluation scores (ROUGE-1, BLEU, METEOR) for summaries generated by different language models compared against GPT-4:

{formatted_scores}

Please analyze the scores and tell which model performed best overall. Your answer should be short (2-3 lines), clear, and reference the model name directly.
"""
    response = llm.invoke(prompt)
    return response.content.strip()